{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36446ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from enum import Enum\n",
    "from io_trace_set import *\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import random\n",
    "\n",
    "\n",
    "class OpType(Enum):\n",
    "    READ = 0\n",
    "    WRITE = 1\n",
    "\n",
    "\n",
    "class ReadWriteType(Enum):\n",
    "    READ_HOT_WRITE_HOT = 0\n",
    "    READ_HOT_WRITE_COLD = 1\n",
    "    READ_COLD_WRITE_HOT = 2\n",
    "    READ_COLD_WRITE_COLD = 3\n",
    "\n",
    "\n",
    "class Freq:\n",
    "\n",
    "    def __init__(self, read: int, write: int):\n",
    "        self.read = read\n",
    "        self.write = write\n",
    "\n",
    "\n",
    "class LeastRecentlyUsed(OrderedDict):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.cache = OrderedDict()\n",
    "\n",
    "    def get(self, key) -> Freq:\n",
    "        if key in self.cache:\n",
    "            value = self.cache.get(key)\n",
    "        else:\n",
    "            value = Freq(0, 0)\n",
    "        return value\n",
    "\n",
    "    def set(self, key, type: OpType, size: int) -> None:\n",
    "        if key in self.cache:\n",
    "            value = self.cache.pop(key)\n",
    "            if type == OpType.READ:\n",
    "                value.read += size\n",
    "            else:\n",
    "                value.write += size\n",
    "            self.cache[key] = value\n",
    "        else:\n",
    "            read = size if OpType == OpType.READ else 0\n",
    "            write = size if OpType == OpType.WRITE else 0\n",
    "            value = Freq(read, write)\n",
    "            if len(self.cache) == self.capacity:\n",
    "                zero_freq_found = False\n",
    "                for k, v in self.cache.items():\n",
    "                    if v.read == 0 and v.write == 0:\n",
    "                        zero_freq_found = True\n",
    "                        self.cache.pop(k)\n",
    "                        self.cache[key] = value\n",
    "                        break\n",
    "                if not zero_freq_found and random.randint(0, 1) == 0:\n",
    "                    # probability of 0.5 to evict the first item\n",
    "                    # if there is no zero freq\n",
    "                    self.cache.popitem(last=False)  # pop出第一个item\n",
    "                    self.cache[key] = value\n",
    "            else:\n",
    "                self.cache[key] = value\n",
    "\n",
    "    def decay(self) -> None:\n",
    "        for _, v in self.cache.items():\n",
    "            if v.read == 1:\n",
    "                v.read = 0\n",
    "            else:\n",
    "                v.read /= 2\n",
    "            if v.write == 1:\n",
    "                v.write = 0\n",
    "            else:\n",
    "                v.write /= 2\n",
    "\n",
    "\n",
    "class GroupLeastRecentyUsed:\n",
    "\n",
    "    def __init__(self, k, capacity, period):\n",
    "        self.k = k\n",
    "        self.lru_group = [LeastRecentlyUsed(capacity) for _ in range(k)]\n",
    "        self.period = period  # 每个周期的请求次数，其中每个周期结束时对lru_group执行一次decay\n",
    "        self.count = 0  # 记录当前周期是第几次请求\n",
    "        self.ratio = 2  # 读写频率比例\n",
    "        self.penalty_ratio = 2  # 预测出错时的在线训练惩罚比率\n",
    "        self.bonus_ratio = 1  # 预测正确时的在线训练奖励比率\n",
    "\n",
    "    def fit(self, trace: IOTrace):\n",
    "        LPN = trace.offset\n",
    "        idx = LPN % self.k\n",
    "        self.lru_group[idx].set(LPN, trace.type, trace.size)\n",
    "\n",
    "        self.count = (self.count + 1) % self.period\n",
    "        if self.count == 0:\n",
    "            pool = ThreadPool()  # 用于并行执行decay操作的线程池\n",
    "            pool.map(lambda x: x.decay(), self.lru_group)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "    def predict(self, trace: IOTrace) -> ReadWriteType:\n",
    "        LPN = trace.offset\n",
    "        idx = LPN % self.k\n",
    "        lru = self.lru_group[idx]\n",
    "\n",
    "        freq = lru.get(LPN)\n",
    "        read, write = freq.read, freq.write\n",
    "\n",
    "        if read == 0 and write == 0:\n",
    "            return ReadWriteType.READ_COLD_WRITE_COLD\n",
    "        if (read + 1) / (write + 1) >= self.ratio:\n",
    "            return ReadWriteType.READ_HOT_WRITE_COLD\n",
    "        elif (write + 1) / (read + 1) >= self.ratio:\n",
    "            return ReadWriteType.READ_COLD_WRITE_HOT\n",
    "        else:\n",
    "            return ReadWriteType.READ_HOT_WRITE_HOT\n",
    "\n",
    "\n",
    "class GroupLeastRecentlyUsedModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.glru = GroupLeastRecentyUsed(\n",
    "            k=100, capacity=100000, period=10000000)\n",
    "\n",
    "    def trace_one(self, trace: IOTrace) -> list:\n",
    "        rwType = self.glru.predict(trace)\n",
    "        if rwType == ReadWriteType.READ_HOT_WRITE_HOT:\n",
    "            cost = 2\n",
    "        elif rwType == ReadWriteType.READ_COLD_WRITE_COLD:\n",
    "            cost = 3\n",
    "        else:\n",
    "            if rwType == ReadWriteType.READ_HOT_WRITE_COLD:\n",
    "                cost = 1 if trace.type == 0 else 3\n",
    "            else:\n",
    "                cost = 1 if trace.type == 1 else 3\n",
    "\n",
    "        # false prediction, add penalty\n",
    "        if cost == 3:\n",
    "            trace.size *= self.glru.penalty_ratio\n",
    "        elif cost == 1:\n",
    "            trace.size *= self.glru.bonus_ratio\n",
    "        # train online\n",
    "        self.glru.fit(trace)\n",
    "\n",
    "        return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc77634",
   "metadata": {},
   "source": [
    "在GLRU基础上的优化点：\n",
    "1. 增加惩罚系数，即当预测出错的时候，增加在线训练时该样本的权重，用以加速新的读写模式的识别\n",
    "2. 记录每个Chunk的读写模式切换的周期和频率等指标，对于频繁切换的Chunk，应该让错误样本的在线训练的惩罚系数值更大，以识别出周期性的模式切换规律\n",
    "3. 根据每个时刻工作集的大小动态调整GLRU列表的大小？（待进一步研究）\n",
    "4. 。。。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d6164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    'hm', 'mds', 'prn', 'proj', 'prxy', 'rsrch',\n",
    "    'src1', 'src2', 'stg', 'ts', 'usr', 'wdev', 'web',\n",
    "]\n",
    "dataset_to_csv_file = lambda d: 'datasets/%s_processed.csv' % d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8304f59",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Write'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m      6\u001b[0m     csv_file \u001b[38;5;241m=\u001b[39m dataset_to_csv_file(dataset)\n\u001b[0;32m----> 8\u001b[0m     trace_set \u001b[38;5;241m=\u001b[39m \u001b[43mIOTraceSet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_cost_opt_for_read\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     glru_model \u001b[38;5;241m=\u001b[39m GroupLeastRecentlyUsedModel()\n\u001b[1;32m     10\u001b[0m     trace_set\u001b[38;5;241m.\u001b[39mreplay(glru_model\u001b[38;5;241m.\u001b[39mtrace_one)\n",
      "File \u001b[0;32m~/workspace/dm-security/io_trace_set.py:32\u001b[0m, in \u001b[0;36mIOTraceSet.__init__\u001b[0;34m(self, trace_file, original_cost)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(trace_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     31\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(f)\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_set \u001b[38;5;241m=\u001b[39m [IOTrace(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace num: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_set))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/workspace/dm-security/io_trace_set.py:32\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(trace_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     31\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(f)\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_set \u001b[38;5;241m=\u001b[39m [\u001b[43mIOTrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace num: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_set))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/workspace/dm-security/io_trace_set.py:8\u001b[0m, in \u001b[0;36mIOTrace.__init__\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, row):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Write'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = [['dataset', 'original_cost_opt',\n",
    "            'original_cost', 'actual_cost', 'save_ratio']]\n",
    "\n",
    "for dataset in datasets:\n",
    "    csv_file = dataset_to_csv_file(dataset)\n",
    "\n",
    "    trace_set = IOTraceSet(csv_file, original_cost_opt_for_read)\n",
    "    glru_model = GroupLeastRecentlyUsedModel()\n",
    "    trace_set.replay(glru_model.trace_one)\n",
    "    trace_set.show_result()\n",
    "    result = [dataset, 'read']\n",
    "    result.extend(trace_set.get_result())\n",
    "    results.append(result)\n",
    "\n",
    "    trace_set = IOTraceSet(csv_file, original_cost_opt_for_write)\n",
    "    glru_model = GroupLeastRecentlyUsedModel()\n",
    "    trace_set.replay(glru_model.trace_one)\n",
    "    trace_set.show_result()\n",
    "    result = [dataset, 'write']\n",
    "    result.extend(trace_set.get_result())\n",
    "    results.append(result)\n",
    "\n",
    "    trace_set = IOTraceSet(csv_file, original_cost_opt_for_balanced)\n",
    "    glru_model = GroupLeastRecentlyUsedModel()\n",
    "    trace_set.replay(glru_model.trace_one)\n",
    "    trace_set.show_result()\n",
    "    result = [dataset, 'balanced']\n",
    "    result.extend(trace_set.get_result())\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc1b1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
